1.需要确认取出的hidden states位置是否正确，默认配置是1，layers//2-1，layers-4，而原配置是2，layers//2，layers-3
    应该都可以，但是需要训推保持一致
2.offline模式是先生成hidden states，减少训练时target model的前向过程，好处是避免训练时加载target model，但是需要存储4倍训练数据
    最好用offline的方式，方便利用算力和断点续训
3.需要确认训练qwen3的draft model结构是仿llama还是qwen3
    仿llama，暂时先按仿llama格式
4.取出hidden state的方式是全量取出所有层的hidden state，再取所需，这样需要保存太多
    offline模式可以不用考虑这一点
5.重点读angleslim/compressor/speculative/train/trainer/eagle3_trainer.py 里面是训练过程，包含loss计算
    angleslim/compressor/speculative/train/models/draft/llama_eagle3.py 里面是draft模型定义
6.理清楚offline训练过程
    offline训练开始前需要先调用tools/generate_hidden_for_draft_model.py生成输入数据，用ckpt保存，
    直接加载输入数据和target model的lm_head，开始训练
    用ckpt格式保存数据可以沿用，但是对话数据格式需要调整。原来使用的是openai对话格式，修改格式需要看
    angleslim/compressor/speculative/train/data/dataset.py(154):_preprocess_function

7.eagle模式，对于某一个模型，需要
    合成数据
    定义eagle模型（可以直接用llama decode layer，除decode layer需要修改，其他保持一致）
    训练draft模型
    部署eagle推理（这个可以直接用通用的框架完成）


8.已修改部分：
    angleslim/compressor/speculative/train/models/draft/llama_eagle3.py：根据qwen3结构，添加qk的归一化
    angleslim/compressor/speculative/train/models/target/target_model_wrapper.py：修改取出的hidden state的层数
    添加qwen3-32B的config配置
    还需要修改拉起训练的脚本，主要是offline训练脚本